{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a971a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# NLP Model Training for SATIM FAQ Bot\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook trains a natural language processing model for the SATIM call center FAQ bot using the scraped data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"from datetime import datetime\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, classification_report\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"from nltk.stem import SnowballStemmer\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download required NLTK data\\n\",\n",
    "    \"nltk.download('punkt', quiet=True)\\n\",\n",
    "    \"nltk.download('stopwords', quiet=True)\\n\",\n",
    "    \"nltk.download('punkt_tab', quiet=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Setup complete!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Scraped Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's load the FAQ data that we scraped in the previous notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load cleaned FAQ data\\n\",\n",
    "    \"df = pd.read_csv('../data/processed/satim_faqs_cleaned.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(df)} FAQs\\\")\\n\",\n",
    "    \"print(f\\\"Categories: {list(df['category'].unique())}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display basic statistics\\n\",\n",
    "    \"print(\\\"\\\\nDataset Overview:\\\")\\n\",\n",
    "    \"print(df.info())\\n\",\n",
    "    \"print(\\\"\\\\nFirst few rows:\\\")\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Text Preprocessing\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create functions to preprocess French text for better NLP performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# French stopwords and stemmer\\n\",\n",
    "    \"french_stopwords = set(stopwords.words('french'))\\n\",\n",
    "    \"stemmer = SnowballStemmer('french')\\n\",\n",
    "    \"\\n\",\n",
    "    \"def preprocess_french_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Preprocess French text for NLP\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    if pd.isna(text) or not isinstance(text, str):\\n\",\n",
    "    \"        return \\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Convert to lowercase\\n\",\n",
    "    \"    text = text.lower()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove special characters but keep French accents\\n\",\n",
    "    \"    text = re.sub(r'[^\\\\w\\\\s\\\\-àâäçéèêëïîôöùûüÿ]', ' ', text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove extra whitespace\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenize\\n\",\n",
    "    \"    tokens = word_tokenize(text, language='french')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove stopwords and short words\\n\",\n",
    "    \"    tokens = [token for token in tokens if token not in french_stopwords and len(token) > 2]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Stem tokens\\n\",\n",
    "    \"    tokens = [stemmer.stem(token) for token in tokens]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return ' '.join(tokens)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test preprocessing function\\n\",\n",
    "    \"sample_text = \\\"Comment puis-je contacter le service client de SATIM pour résoudre mon problème?\\\"\\n\",\n",
    "    \"processed_text = preprocess_french_text(sample_text)\\n\",\n",
    "    \"print(f\\\"Original: {sample_text}\\\")\\n\",\n",
    "    \"print(f\\\"Processed: {processed_text}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Prepare Training Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's preprocess all our FAQ data and prepare it for model training.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Preprocess questions and answers\\n\",\n",
    "    \"print(\\\"Preprocessing text data...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"df['processed_question'] = df['question'].apply(preprocess_french_text)\\n\",\n",
    "    \"df['processed_answer'] = df['answer'].apply(preprocess_french_text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove rows with empty processed text\\n\",\n",
    "    \"df = df[(df['processed_question'].str.len() > 0) & (df['processed_answer'].str.len() > 0)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"After preprocessing: {len(df)} FAQs\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show some examples\\n\",\n",
    "    \"print(\\\"\\\\nPreprocessing examples:\\\")\\n\",\n",
    "    \"for i in range(3):\\n\",\n",
    "    \"    print(f\\\"\\\\n--- Example {i+1} ---\\\")\\n\",\n",
    "    \"    print(f\\\"Original Q: {df.iloc[i]['question']}\\\")\\n\",\n",
    "    \"    print(f\\\"Processed Q: {df.iloc[i]['processed_question']}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Build TF-IDF Vectorizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"We'll use TF-IDF to convert text into numerical vectors for similarity matching.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create TF-IDF vectorizer optimized for French\\n\",\n",
    "    \"vectorizer = TfidfVectorizer(\\n\",\n",
    "    \"    max_features=5000,\\n\",\n",
    "    \"    ngram_range=(1, 2),  # Use unigrams and bigrams\\n\",\n",
    "    \"    min_df=2,  # Ignore terms that appear in less than 2 documents\\n\",\n",
    "    \"    max_df=0.8,  # Ignore terms that appear in more than 80% of documents\\n\",\n",
    "    \"    sublinear_tf=True\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fit vectorizer on processed questions\\n\",\n",
    "    \"question_vectors = vectorizer.fit_transform(df['processed_question'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"TF-IDF matrix shape: {question_vectors.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Vocabulary size: {len(vectorizer.vocabulary_)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show most important features\\n\",\n",
    "    \"feature_names = vectorizer.get_feature_names_out()\\n\",\n",
    "    \"print(f\\\"\\\\nSample features: {feature_names[:20]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Create FAQ Similarity Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create a model that can find the most similar FAQ for any given question.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class FAQSimilarityModel:\\n\",\n",
    "    \"    def __init__(self, vectorizer, question_vectors, faq_data):\\n\",\n",
    "    \"        self.vectorizer = vectorizer\\n\",\n",
    "    \"        self.question_vectors = question_vectors\\n\",\n",
    "    \"        self.faq_data = faq_data.reset_index(drop=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    def find_best_match(self, query, top_k=3, min_similarity=0.1):\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Find the best matching FAQ for a given query\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        # Preprocess query\\n\",\n",
    "    \"        processed_query = preprocess_french_text(query)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if not processed_query:\\n\",\n",
    "    \"            return []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Vectorize query\\n\",\n",
    "    \"        query_vector = self.vectorizer.transform([processed_query])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Calculate similarities\\n\",\n",
    "    \"        similarities = cosine_similarity(query_vector, self.question_vectors).flatten()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get top matches\\n\",\n",
    "    \"        top_indices = similarities.argsort()[-top_k:][::-1]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        results = []\\n\",\n",
    "    \"        for idx in top_indices:\\n\",\n",
    "    \"            similarity = similarities[idx]\\n\",\n",
    "    \"            if similarity >= min_similarity:\\n\",\n",
    "    \"                results.append({\\n\",\n",
    "    \"                    'question': self.faq_data.iloc[idx]['question'],\\n\",\n",
    "    \"                    'answer': self.faq_data.iloc[idx]['answer'],\\n\",\n",
    "    \"                    'category': self.faq_data.iloc[idx]['category'],\\n\",\n",
    "    \"                    'similarity': float(similarity),\\n\",\n",
    "    \"                    'confidence': self.calculate_confidence(similarity)\\n\",\n",
    "    \"                })\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def calculate_confidence(self, similarity):\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Convert similarity score to confidence level\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        if similarity >= 0.8:\\n\",\n",
    "    \"            return 'high'\\n\",\n",
    "    \"        elif similarity >= 0.5:\\n\",\n",
    "    \"            return 'medium'\\n\",\n",
    "    \"        elif similarity >= 0.2:\\n\",\n",
    "    \"            return 'low'\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            return 'very_low'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create the model\\n\",\n",
    "    \"faq_model = FAQSimilarityModel(vectorizer, question_vectors, df)\\n\",\n",
    "    \"print(\\\"✓ FAQ Similarity Model created successfully\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Test the Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's test our model with various queries to see how well it performs.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test queries\\n\",\n",
    "    \"test_queries = [\\n\",\n",
    "    \"    \\\"Comment contacter SATIM?\\\",\\n\",\n",
    "    \"    \\\"Quels sont vos horaires d'ouverture?\\\",\\n\",\n",
    "    \"    \\\"Comment faire un paiement?\\\",\\n\",\n",
    "    \"    \\\"J'ai un problème technique\\\",\\n\",\n",
    "    \"    \\\"Où êtes-vous situés?\\\",\\n\",\n",
    "    \"    \\\"Comment créer un compte?\\\",\\n\",\n",
    "    \"    \\\"Problème avec ma carte\\\",\\n\",\n",
    "    \"    \\\"Tarifs et frais\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Testing FAQ Model:\\\\n\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 80)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, query in enumerate(test_queries, 1):\\n\",\n",
    "    \"    print(f\\\"\\\\n🔍 Test Query {i}: '{query}'\\\")\\n\",\n",
    "    \"    print(\\\"-\\\" * 50)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results = faq_model.find_best_match(query, top_k=2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if results:\\n\",\n",
    "    \"        for j, result in enumerate(results, 1):\\n\",\n",
    "    \"            print(f\\\"\\\\n  Match {j} (Similarity: {result['similarity']:.3f}, Confidence: {result['confidence']})\\\")\\n\",\n",
    "    \"            print(f\\\"  Q: {result['question']}\\\")\\n\",\n",
    "    \"            print(f\\\"  A: {result['answer'][:150]}...\\\")\\n\",\n",
    "    \"            print(f\\\"  Category: {result['category']}\\\")\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"  ❌ No suitable matches found\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n\\\" + \\\"=\\\" * 80)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Evaluate Model Performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create a more systematic evaluation of our model's performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create evaluation dataset by using existing questions\\n\",\n",
    "    \"# We'll modify them slightly to simulate real user queries\\n\",\n",
    "    \"\\n\",\n",
    "    \"def create_evaluation_queries(df, n_samples=50):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create evaluation queries by modifying existing questions\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    eval_data = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Sample questions\\n\",\n",
    "    \"    sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for idx, row in sample_df.iterrows():\\n\",\n",
    "    \"        original_question = row['question']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create variations\\n\",\n",
    "    \"        variations = [\\n\",\n",
    "    \"            original_question,  # Exact match\\n\",\n",
    "    \"            original_question.replace('Comment', 'Comment puis-je'),\\n\",\n",
    "    \"            original_question.replace('?', ' s\\\\'il vous plaît?'),\\n\",\n",
    "    \"            original_question.lower(),\\n\",\n",
    "    \"            ' '.join(original_question.split()[:5]) + '?'  # Truncated\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for variation in variations:\\n\",\n",
    "    \"            eval_data.append({\\n\",\n",
    "    \"                'query': variation,\\n\",\n",
    "    \"                'expected_idx': idx,\\n\",\n",
    "    \"                'expected_question': original_question,\\n\",\n",
    "    \"                'expected_category': row['category']\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return eval_data\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create evaluation dataset\\n\",\n",
    "    \"eval_data = create_evaluation_queries(df, n_samples=20)\\n\",\n",
    "    \"print(f\\\"Created {len(eval_data)} evaluation queries\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show some examples\\n\",\n",
    "    \"print(\\\"\\\\nEvaluation query examples:\\\")\\n\",\n",
    "    \"for i in range(3):\\n\",\n",
    "    \"    print(f\\\"Query: {eval_data[i]['query']}\\\")\\n\",\n",
    "    \"    print(f\\\"Expected: {eval_data[i]['expected_question']}\\\")\\n\",\n",
    "    \"    print()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate model performance\\n\",\n",
    "    \"def evaluate_model(model, eval_data):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Evaluate the FAQ model performance\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    results = {\\n\",\n",
    "    \"        'top1_accuracy': 0,\\n\",\n",
    "    \"        'top3_accuracy': 0,\\n\",\n",
    "    \"        'avg_similarity': 0,\\n\",\n",
    "    \"        'no_match_rate': 0,\\n\",\n",
    "    \"        'confidence_distribution': {'high': 0, 'medium': 0, 'low': 0, 'very_low': 0}\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    top1_correct = 0\\n\",\n",
    "    \"    top3_correct = 0\\n\",\n",
    "    \"    total_similarity = 0\\n\",\n",
    "    \"    no_matches = 0\\n\",\n",
    "    \"    valid_queries = 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for eval_item in eval_data:\\n\",\n",
    "    \"        query = eval_item['query']\\n\",\n",
    "    \"        expected_question = eval_item['expected_question']\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        matches = model.find_best_match(query, top_k=3, min_similarity=0.05)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if matches:\\n\",\n",
    "    \"            valid_queries += 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Check top-1 accuracy\\n\",\n",
    "    \"            if matches[0]['question'] == expected_question:\\n\",\n",
    "    \"                top1_correct += 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Check top-3 accuracy\\n\",\n",
    "    \"            if any(match['question'] == expected_question for match in matches):\\n\",\n",
    "    \"                top3_correct += 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Record similarity and confidence\\n\",\n",
    "    \"            total_similarity += matches[0]['similarity']\\n\",\n",
    "    \"            results['confidence_distribution'][matches[0]['confidence']] += 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            no_matches += 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate metrics\\n\",\n",
    "    \"    total_queries = len(eval_data)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if valid_queries > 0:\\n\",\n",
    "    \"        results['top1_accuracy'] = top1_correct / total_queries\\n\",\n",
    "    \"        results['top3_accuracy'] = top3_correct / total_queries\\n\",\n",
    "    \"        results['avg_similarity'] = total_similarity / valid_queries\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results['no_match_rate'] = no_matches / total_queries\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run evaluation\\n\",\n",
    "    \"print(\\\"Evaluating model performance...\\\")\\n\",\n",
    "    \"performance = evaluate_model(faq_model, eval_data)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n📊 Model Performance Results:\\\")\\n\",\n",
    "    \"print(f\\\"Top-1 Accuracy: {performance['top1_accuracy']:.3f} ({performance['top1_accuracy']*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"Top-3 Accuracy: {performance['top3_accuracy']:.3f} ({performance['top3_accuracy']*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"Average Similarity: {performance['avg_similarity']:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"No Match Rate: {performance['no_match_rate']:.3f} ({performance['no_match_rate']*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n🎯 Confidence Distribution:\\\")\\n\",\n",
    "    \"for confidence, count in performance['confidence_distribution'].items():\\n\",\n",
    "    \"    print(f\\\"  {confidence}: {count} queries\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Model Analysis and Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze our model's performance and create some visualizations.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze similarity score distribution\\n\",\n",
    "    \"all_similarities = []\\n\",\n",
    "    \"sample_queries = df['question'].sample(100, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for query in sample_queries:\\n\",\n",
    "    \"    matches = faq_model.find_best_match(query, top_k=1, min_similarity=0.0)\\n\",\n",
    "    \"    if matches:\\n\",\n",
    "    \"        all_similarities.append(matches[0]['similarity'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create visualizations\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n",
    "    \"fig.suptitle('SATIM FAQ Model Analysis', fontsize=16, fontweight='bold')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Similarity distribution\\n\",\n",
    "    \"axes[0, 0].hist(all_similarities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\\n\",\n",
    "    \"axes[0, 0].set_title('Distribution of Similarity Scores')\\n\",\n",
    "    \"axes[0, 0].set_xlabel('Similarity Score')\\n\",\n",
    "    \"axes[0, 0].set_ylabel('Frequency')\\n\",\n",
    "    \"axes[0, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Category distribution in dataset\\n\",\n",
    "    \"category_counts = df['category'].value_counts()\\n\",\n",
    "    \"colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\\n\",\n",
    "    \"axes[0, 1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', colors=colors)\\n\",\n",
    "    \"axes[0, 1].set_title('FAQ Categories Distribution')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Question length distribution\\n\",\n",
    "    \"question_lengths = df['question'].str.len()\\n\",\n",
    "    \"axes[1, 0].hist(question_lengths, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\\n\",\n",
    "    \"axes[1, 0].set_title('Question Length Distribution')\\n\",\n",
    "    \"axes[1, 0].set_xlabel('Characters')\\n\",\n",
    "    \"axes[1, 0].set_ylabel('Frequency')\\n\",\n",
    "    \"axes[1, 0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Performance metrics visualization\\n\",\n",
    "    \"metrics = ['Top-1 Acc', 'Top-3 Acc', 'Avg Similarity', 'Coverage']\\n\",\n",
    "    \"values = [\\n\",\n",
    "    \"    performance['top1_accuracy'], \\n\",\n",
    "    \"    performance['top3_accuracy'], \\n\",\n",
    "    \"    performance['avg_similarity'],\\n\",\n",
    "    \"    1 - performance['no_match_rate']\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"bars = axes[1, 1].bar(metrics, values, alpha=0.7, color=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99'])\\n\",\n",
    "    \"axes[1, 1].set_title('Model Performance Metrics')\\n\",\n",
    "    \"axes[1, 1].set_ylabel('Score')\\n\",\n",
    "    \"axes[1, 1].set_ylim(0, 1)\\n\",\n",
    "    \"axes[1, 1].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels on bars\\n\",\n",
    "    \"for bar, value in zip(bars, values):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\\n\",\n",
    "    \"                    f'{value:.3f}', ha='center', va='bottom')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print detailed statistics\\n\",\n",
    "    \"print(\\\"\\\\n📈 Detailed Statistics:\\\")\\n\",\n",
    "    \"print(f\\\"Total FAQs: {len(df)}\\\")\\n\",\n",
    "    \"print(f\\\"Average question length: {question_lengths.mean():.1f} characters\\\")\\n\",\n",
    "    \"print(f\\\"Median question length: {question_lengths.median():.1f} characters\\\")\\n\",\n",
    "    \"print(f\\\"Average similarity score: {np.mean(all_similarities):.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Standard deviation of similarities: {np.std(all_similarities):.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Category-Based Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze model performance by FAQ category.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Analyze performance by category\\n\",\n",
    "    \"def analyze_by_category(model, df):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Analyze model performance by FAQ category\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    category_performance = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for category in df['category'].unique():\\n\",\n",
    "    \"        category_df = df[df['category'] == category]\\n\",\n",
    "    \"        category_queries = category_df['question'].sample(min(10, len(category_df)), random_state=42)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        similarities = []\\n\",\n",
    "    \"        correct_matches = 0\\n\",\n",
    "    \"        total_queries = len(category_queries)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for query in category_queries:\\n\",\n",
    "    \"            matches = model.find_best_match(query, top_k=1, min_similarity=0.0)\\n\",\n",
    "    \"            if matches:\\n\",\n",
    "    \"                similarities.append(matches[0]['similarity'])\\n\",\n",
    "    \"                if matches[0]['category'] == category:\\n\",\n",
    "    \"                    correct_matches += 1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        category_performance[category] = {\\n\",\n",
    "    \"            'avg_similarity': np.mean(similarities) if similarities else 0,\\n\",\n",
    "    \"            'category_accuracy': correct_matches / total_queries if total_queries > 0 else 0,\\n\",\n",
    "    \"            'total_faqs': len(category_df),\\n\",\n",
    "    \"            'queries_tested': total_queries\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return category_performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Perform category analysis\\n\",\n",
    "    \"category_perf = analyze_by_category(faq_model, df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n📊 Performance by Category:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 70)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for category, metrics in category_perf.items():\\n\",\n",
    "    \"    print(f\\\"\\\\n📁 {category}:\\\")\\n\",\n",
    "    \"    print(f\\\"  Total FAQs: {metrics['total_faqs']}\\\")\\n\",\n",
    "    \"    print(f\\\"  Average Similarity: {metrics['avg_similarity']:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"  Category Accuracy: {metrics['category_accuracy']:.3f} ({metrics['category_accuracy']*100:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"  Queries Tested: {metrics['queries_tested']}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize category performance\\n\",\n",
    "    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"categories = list(category_perf.keys())\\n\",\n",
    "    \"similarities = [category_perf[cat]['avg_similarity'] for cat in categories]\\n\",\n",
    "    \"accuracies = [category_perf[cat]['category_accuracy'] for cat in categories]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Average similarity by category\\n\",\n",
    "    \"bars1 = ax1.bar(categories, similarities, alpha=0.7, color='lightblue')\\n\",\n",
    "    \"ax1.set_title('Average Similarity by Category')\\n\",\n",
    "    \"ax1.set_ylabel('Average Similarity')\\n\",\n",
    "    \"ax1.tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"ax1.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add value labels\\n\",\n",
    "    \"for bar, value in zip(bars1, similarities):\\n\",\n",
    "    \"    height = bar.get_height()\\n\",\n",
    "    \"    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\\n\",\n",
    "    \"            f'{value:.3f}', ha='center', va='bottom')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Category accuracy\\n\",\n",
    "    \"bars2 = ax2.bar(categories, accuracies, alpha=0.7, color='lightcoral')\\n\","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
